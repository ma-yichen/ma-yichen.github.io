I"Î2<h1 id="research">Research</h1>

<div class="rowl1">
  <p><img src="http://localhost:4000/images/respic/droplets_in_water.jpeg" class="img-responsive" width="20%" style="float: left; border-radius:10px" /></p>
  <h4>Bubble and droplet dynamics</h4>
  <p>Ensemble- and volume-averaging are phase-averaged methods for disperse, bubbly flows. While built upon similar assumptions, it is challenging to assess their relative merits. Volume-averaging is an intrinsically deterministic model, for which bubbles are represented in a Lagrangian framework as advected particles, each sampled from a distribution of equilibrium bubble sizes. The dynamic coupling to the liquid phase is modeled through local volume averaging. Ensemble-phase averaging is stochastic, and uses ensemble-averaging to derive mixture-averaged equations and the field equations are evolved in an Eulerian reference frame for the associated bubble properties, each representing bins of an underlying equilibrium distribution. In both cases the equations are closed by solving Rayleigh-Plesset-like equations for the bubble dynamics as forced by the local or mixture-averaged pressure, respectively. Computationally, there are complex tradeoffs between these two approaches, especially for modern, parallel architectures. We assess their relative complexity and cost via high-resolution simulations.</p>
  <ul style="overflow: hidden">
  </ul>
</div>

<h1 id="publications">Publications</h1>

<h2 id="journal-papers-and-proceedings">Journal Papers and Proceedings</h2>

<div class="well-sm">
  <ul class="flex-container">
<li class="flex-item1">
  
   <img src="http://localhost:4000/images/pubpic/pickering2022discovery.png" class="img-responsive" width="200%" style="float: left" />
  
</li>
<li class="flex-item2">
  <strong> Discovering and forecasting extreme events via active learning in neural operators</strong> <br />
  <em>E. Pickering, S. Guth, G. Em Karniadakis, T.P. Sapsis </em><br />
  Nature Compuational Science (2022)<br />
  <a href="http://localhost:4000/papers/pickering2022discovery.pdf" target="_blank"><button class="btn-pdf">PDF</button></a>
  <a href="http://dx.doi.org/10.1038/s43588-022-00376-0" target="_blank"><button class="btn-doi">DOI</button></a> 
  
   <a data-toggle="collapse" href="#pickering2022discovery2" class="btn-bib" style="text-decoration:none; color:#ebebeb; hover:#ebebeb;" role="button" aria-expanded="false" aria-controls="pickering2022discovery2">BIB</a> 
   <a data-toggle="collapse" href="#pickering2022discovery" class="btn-abstract" style="text-decoration:none; color:#ebebeb; hover:#ebebeb;" role="button" aria-expanded="false" aria-controls="pickering2022discovery">ABSTRACT</a>


<br />
<div class="collapse" id="pickering2022discovery"><div class="well-abstract">
 Extreme events in society and nature, such as pandemic spikes, rogue waves or structural failures, can have catastrophic consequences. Characterizing extremes is difficult, as they occur rarely, arise from seemingly benign conditions, and belong to complex and often unknown infinite-dimensional systems. Such challenges render attempts at characterizing them moot. We address each of these difficulties by combining output-weighted training schemes in Bayesian experimental design (BED) with an ensemble of deep neural operators. This model-agnostic framework pairs a BED scheme that actively selects data for quantifying extreme events with an ensemble of deep neural operators that approximate infinite-dimensional nonlinear operators. We show that not only does this framework outperform Gaussian processes, but that (1) shallow ensembles of just two members perform best; (2) extremes are uncovered regardless of the state of the initial data (that is, with or without extremes); (3) our method eliminates ‚Äòdouble-descent‚Äô phenomena; (4) the use of batches of suboptimal acquisition samples compared to step-by-step global optima does not hinder BED performance; and (5) Monte Carlo acquisition outperforms standard optimizers in high dimensions. Together, these conclusions form a scalable artificial intelligence (AI)-assisted experimental infrastructure that can efficiently infer and pinpoint critical situations across many domains, from physical to societal systems.
</div></div>



<div class="collapse" id="pickering2022discovery2"><div class="well-bib">
<iframe src="http://localhost:4000/papers/pickering2022discovery.txt" scrolling="yes" width="100%" height="210" frameborder="0"></iframe>
</div></div>


</li>
</ul>

</div>

<div class="well-sm">
  <ul class="flex-container">
<li class="flex-item1">
  
   <img src="http://localhost:4000/images/pubpic/pickering2022fomo.png" class="img-responsive" width="200%" style="float: left" />
  
</li>
<li class="flex-item2">
  <strong> Information FOMO, The unhealthy fear of missing out on information. A method for removing misleading data for healthier models</strong> <br />
  <em>E. Pickering, T.P. Sapsis </em><br />
  <b>Submitted</b> (2022)<br />
  <a href="http://localhost:4000/papers/pickering2022fomo.pdf" target="_blank"><button class="btn-pdf">PDF</button></a>
  
  <a href="https://arxiv.org/abs/2208.1308" target="_blank"><button class="btn-arxiv">ARXIV</button></a> 
   <a data-toggle="collapse" href="#pickering2022fomo2" class="btn-bib" style="text-decoration:none; color:#ebebeb; hover:#ebebeb;" role="button" aria-expanded="false" aria-controls="pickering2022fomo2">BIB</a> 
   <a data-toggle="collapse" href="#pickering2022fomo" class="btn-abstract" style="text-decoration:none; color:#ebebeb; hover:#ebebeb;" role="button" aria-expanded="false" aria-controls="pickering2022fomo">ABSTRACT</a>


<br />
<div class="collapse" id="pickering2022fomo"><div class="well-abstract">
 Misleading or unnecessary data can have out-sized impacts on the health or accuracy of Machine Learning (ML) models. We present a Bayesian sequential selection method, akin to Bayesian experimental design, that identifies critically important information within a dataset, while ignoring data that is either misleading or brings unnecessary complexity to the surrogate model of choice. Specifically, our method eliminates the phenomena of double descent, where more data leads to worse performance. Our approach has two key features. First, the selection algorithm dynamically couples the chosen model and data. Data is chosen based on its merits towards improving the selected model, rather than being compared strictly against other data. Second, a natural convergence of the method removes the need for dividing the data into training, testing, and validation sets. Instead, the selection metric inherently assesses testing and validation error through global statistics of the model. This ensures that key information is never wasted in testing or validation. The method is applied using both Gaussian process regression and deep neural network surrogate models.
</div></div>



<div class="collapse" id="pickering2022fomo2"><div class="well-bib">
<iframe src="http://localhost:4000/papers/pickering2022fomo.txt" scrolling="yes" width="100%" height="210" frameborder="0"></iframe>
</div></div>


</li>
</ul>

</div>

<hr />
<p>title: ‚ÄúPublications‚Äù
layout: gridlay
sitemap: false
permalink: /publications/
‚Äî</p>

<h1 id="publications-1">Publications</h1>

<h2 id="journal-papers-and-proceedings-1">Journal Papers and Proceedings</h2>

<div class="well-sm">
  <ul class="flex-container">
<li class="flex-item1">
  
   <img src="http://localhost:4000/images/pubpic/pickering2022discovery.png" class="img-responsive" width="200%" style="float: left" />
  
</li>
<li class="flex-item2">
  <strong> Discovering and forecasting extreme events via active learning in neural operators</strong> <br />
  <em>E. Pickering, S. Guth, G. Em Karniadakis, T.P. Sapsis </em><br />
  Nature Compuational Science (2022)<br />
  <a href="http://localhost:4000/papers/pickering2022discovery.pdf" target="_blank"><button class="btn-pdf">PDF</button></a>
  <a href="http://dx.doi.org/10.1038/s43588-022-00376-0" target="_blank"><button class="btn-doi">DOI</button></a> 
  
   <a data-toggle="collapse" href="#pickering2022discovery2" class="btn-bib" style="text-decoration:none; color:#ebebeb; hover:#ebebeb;" role="button" aria-expanded="false" aria-controls="pickering2022discovery2">BIB</a> 
   <a data-toggle="collapse" href="#pickering2022discovery" class="btn-abstract" style="text-decoration:none; color:#ebebeb; hover:#ebebeb;" role="button" aria-expanded="false" aria-controls="pickering2022discovery">ABSTRACT</a>


<br />
<div class="collapse" id="pickering2022discovery"><div class="well-abstract">
 Extreme events in society and nature, such as pandemic spikes, rogue waves or structural failures, can have catastrophic consequences. Characterizing extremes is difficult, as they occur rarely, arise from seemingly benign conditions, and belong to complex and often unknown infinite-dimensional systems. Such challenges render attempts at characterizing them moot. We address each of these difficulties by combining output-weighted training schemes in Bayesian experimental design (BED) with an ensemble of deep neural operators. This model-agnostic framework pairs a BED scheme that actively selects data for quantifying extreme events with an ensemble of deep neural operators that approximate infinite-dimensional nonlinear operators. We show that not only does this framework outperform Gaussian processes, but that (1) shallow ensembles of just two members perform best; (2) extremes are uncovered regardless of the state of the initial data (that is, with or without extremes); (3) our method eliminates ‚Äòdouble-descent‚Äô phenomena; (4) the use of batches of suboptimal acquisition samples compared to step-by-step global optima does not hinder BED performance; and (5) Monte Carlo acquisition outperforms standard optimizers in high dimensions. Together, these conclusions form a scalable artificial intelligence (AI)-assisted experimental infrastructure that can efficiently infer and pinpoint critical situations across many domains, from physical to societal systems.
</div></div>



<div class="collapse" id="pickering2022discovery2"><div class="well-bib">
<iframe src="http://localhost:4000/papers/pickering2022discovery.txt" scrolling="yes" width="100%" height="210" frameborder="0"></iframe>
</div></div>


</li>
</ul>

</div>

<div class="well-sm">
  <ul class="flex-container">
<li class="flex-item1">
  
   <img src="http://localhost:4000/images/pubpic/pickering2022fomo.png" class="img-responsive" width="200%" style="float: left" />
  
</li>
<li class="flex-item2">
  <strong> Information FOMO, The unhealthy fear of missing out on information. A method for removing misleading data for healthier models</strong> <br />
  <em>E. Pickering, T.P. Sapsis </em><br />
  <b>Submitted</b> (2022)<br />
  <a href="http://localhost:4000/papers/pickering2022fomo.pdf" target="_blank"><button class="btn-pdf">PDF</button></a>
  
  <a href="https://arxiv.org/abs/2208.1308" target="_blank"><button class="btn-arxiv">ARXIV</button></a> 
   <a data-toggle="collapse" href="#pickering2022fomo2" class="btn-bib" style="text-decoration:none; color:#ebebeb; hover:#ebebeb;" role="button" aria-expanded="false" aria-controls="pickering2022fomo2">BIB</a> 
   <a data-toggle="collapse" href="#pickering2022fomo" class="btn-abstract" style="text-decoration:none; color:#ebebeb; hover:#ebebeb;" role="button" aria-expanded="false" aria-controls="pickering2022fomo">ABSTRACT</a>


<br />
<div class="collapse" id="pickering2022fomo"><div class="well-abstract">
 Misleading or unnecessary data can have out-sized impacts on the health or accuracy of Machine Learning (ML) models. We present a Bayesian sequential selection method, akin to Bayesian experimental design, that identifies critically important information within a dataset, while ignoring data that is either misleading or brings unnecessary complexity to the surrogate model of choice. Specifically, our method eliminates the phenomena of double descent, where more data leads to worse performance. Our approach has two key features. First, the selection algorithm dynamically couples the chosen model and data. Data is chosen based on its merits towards improving the selected model, rather than being compared strictly against other data. Second, a natural convergence of the method removes the need for dividing the data into training, testing, and validation sets. Instead, the selection metric inherently assesses testing and validation error through global statistics of the model. This ensures that key information is never wasted in testing or validation. The method is applied using both Gaussian process regression and deep neural network surrogate models.
</div></div>



<div class="collapse" id="pickering2022fomo2"><div class="well-bib">
<iframe src="http://localhost:4000/papers/pickering2022fomo.txt" scrolling="yes" width="100%" height="210" frameborder="0"></iframe>
</div></div>


</li>
</ul>

</div>

:ET